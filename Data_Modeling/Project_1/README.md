## Project 1 - Data Modeling with Postgres
![banner](image/postgres.png)

#### Description
A startup called **Sparkify** wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. The objective of this project is to create a **database schema and ETL pipeline** for song play analysis. In this data modeling project, I performed the following:

- Defined fact and dimension tables for a star schema for a particular analytic focus
- Developed an ETL pipeline that transfers data from files in two directories into these tables in Postgres using Python and SQL

#### Database design
![image](image/Song_ERD.png)

Fact table
- songplays - records in log data associated with song plays
  - songplay_id INTERGER PRIMARY KEY
  - start_time TIMESTAMP
  - user_id INTEGER
  - level VARCHAR(20)
  - song_id VARCHAR(20)
  - artist_id VARCHAR(20)
  - location VARCHAR(80)
  - user_agent VARCHAR(200)

Dimensin tables
- users - users in the app
  - user_id INTEGER PRIMARY KEY
  - first_name VARCHAR(50)
  - last_name VARCHAR(50)
  - gender VARCHAR(1)
  - level VARCHAR(10)
- songs - songs in music database
  - song_id VARCHAR(30)
  - title VARCHAR(100)
  - artist_id VARCHAR(40)
  - year INTEGER
  - duration FLOAT(5)
- artists - artists in music database
  - artist_id VARCHAR(30) PRIMARY KEY
  - name VARCHAR(100)
  - location VARCHAR(100)
  - latitude FLOAT(5)
  - longitude FLOAT(5)
- time - timestamps of records in songplays broken down into specific units
  - start_time TIMESTAMP PRIMARY KEY
  - hour INTEGER
  - day INTEGER
  - week INTEGER
  - month INTEGER
  - year INTEGER
  - weekday INTEGER

#### The ETL pipeline
The ETL pipeline transfers data from files (song dataset and log dataset) in two directories into database tables in Postgres using Python and SQL.
- The song dataset
    - a subset of the [Million Song Dataset](http://millionsongdataset.com/)
    - each file is in JSON format and contain metadata about a song and the artist of that song
    - the files are partitioned by the first three letters of each song's track ID
- The log dataset
    - log files in JSON generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset
    - the files are partitioned by year and month

#### Relevant files
- **create_tables.py** drops and creates the data tables
- **sql_queries.py** contains all sql queries and can be imported to the ETL pipeline files below
- **etl.ipynb** reads and processes single line of file from song_data and log_data to test the sql queries and the ETL process
- **etl.py** contains the ETL pipeline that reads and processes the entire song_data and log_data into the relational tables
- **test.ipynb** is a test file for the ETL pipeline

#### To run the project...
1. In terminal, run ```create_tables.py``` to initilize database and create relevant tables per sql queries in the ```sql_queries.py``` file.
2. In terminal, run ```etl.py``` to extract data in JSON format, transform it, and load to data tables.
3. Use ```test.ipynb``` to check if data is imported correctly.
